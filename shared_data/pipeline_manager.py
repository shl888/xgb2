"""
PipelineManager é¡¶é…ç‰ˆ - æ•°æ®æ€»æŒ‡æŒ¥å®˜
åŠŸèƒ½ï¼šåè°ƒ5æ­¥æµæ°´çº¿ + è´¦æˆ·æ•°æ®ç›´è¿ + æ™ºèƒ½ç›‘æ§ + è‡ªåŠ¨å®¹é”™
è®¾è®¡åŸåˆ™ï¼šç”Ÿäº§çº§å¥å£®æ€§ã€å¯è§‚æµ‹æ€§ã€é«˜ååã€é›¶é˜»å¡
"""

import asyncio
from typing import Dict, Any, List, Optional, Set, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging
import time
from collections import deque
from enum import Enum

# å¯¼å…¥5ä¸ªæ­¥éª¤
from step1_filter import Step1Filter, ExtractedData
from step2_fusion import Step2Fusion, FusedData
from step3_align import Step3Align, AlignedData
from step4_calc import Step4Calc, PlatformData
from step5_cross_calc import Step5CrossCalc, CrossPlatformData

logger = logging.getLogger(__name__)

class DataType(Enum):
    """æ•°æ®ç±»å‹æšä¸¾ï¼ˆä¸¥æ ¼åˆ†ç±»ï¼‰"""
    # å¸‚åœºæ•°æ®ï¼ˆèµ°å®Œæ•´æµæ°´çº¿ï¼‰
    MARKET_TICKER = "ticker"
    MARKET_FUNDING_RATE = "funding_rate"
    MARKET_MARK_PRICE = "mark_price"
    MARKET_OKX_TICKER = "okx_ticker"
    MARKET_OKX_FUNDING = "okx_funding_rate"
    MARKET_BINANCE_TICKER = "binance_ticker"
    MARKET_BINANCE_MARK = "binance_mark_price"
    MARKET_BINANCE_SETTLEMENT = "binance_funding_settlement"
    
    # è´¦æˆ·æ•°æ®ï¼ˆç›´è¿å¤§è„‘ï¼‰
    ACCOUNT_BALANCE = "account"
    ACCOUNT_POSITION = "position"
    ACCOUNT_ORDER = "order"
    ACCOUNT_TRADE = "trade"
    
    # ç³»ç»Ÿæ•°æ®ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
    SYSTEM_STATUS = "connection_status"

@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®ï¼ˆå¯çƒ­æ›´æ–°ï¼‰"""
    # æ‰¹å¤„ç†é˜ˆå€¼
    step1_batch_size: int = 15      # Step1ç¼“å­˜15æ¡å¤„ç†ä¸€æ¬¡
    step2_batch_size: int = 30      # Step2ç¼“å­˜30æ¡å¤„ç†ä¸€æ¬¡
    step3_batch_size: int = 30      # Step3ç¼“å­˜30æ¡å¤„ç†ä¸€æ¬¡
    step4_batch_size: int = 50      # Step4ç¼“å­˜50æ¡å¤„ç†ä¸€æ¬¡
    
    # æ€§èƒ½è°ƒä¼˜
    max_queue_size: int = 10000     # é˜Ÿåˆ—ä¸Šé™ï¼ˆé˜²å†…å­˜çˆ†ï¼‰
    cleanup_interval: int = 300     # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
    metrics_report_interval: int = 60  # æŒ‡æ ‡æŠ¥å‘Šé—´éš”ï¼ˆç§’ï¼‰
    
    # å®¹é”™é…ç½®
    max_retry_attempts: int = 3     # å•æ¡æ•°æ®æœ€å¤§é‡è¯•
    dead_letter_enabled: bool = True  # æ˜¯å¦å¯ç”¨æ­»ä¿¡é˜Ÿåˆ—

@dataclass
class PipelineMetrics:
    """è¿è¡ŒæŒ‡æ ‡ï¼ˆå®æ—¶æ›´æ–°ï¼‰"""
    # å¤„ç†é‡
    step1_processed: int = 0
    step2_processed: int = 0
    step3_processed: int = 0
    step4_processed: int = 0
    step5_processed: int = 0
    direct_processed: int = 0  # ç›´è¿å¤§è„‘çš„æ•°æ®é‡
    
    # é”™è¯¯ç»Ÿè®¡
    step1_errors: int = 0
    step2_errors: int = 0
    step3_errors: int = 0
    step4_errors: int = 0
    step5_errors: int = 0
    
    # æ€§èƒ½æŒ‡æ ‡
    avg_latency_ms: Dict[str, float] = field(default_factory=dict)  # å„æ­¥éª¤å¹³å‡å»¶è¿Ÿ
    queue_size_history: deque = field(default_factory=lambda: deque(maxlen=100))  # é˜Ÿåˆ—å†å²
    last_update: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "processed": {
                "step1": self.step1_processed,
                "step2": self.step2_processed,
                "step3": self.step3_processed,
                "step4": self.step4_processed,
                "step5": self.step5_processed,
                "direct": self.direct_processed
            },
            "errors": {
                "step1": self.step1_errors,
                "step2": self.step2_errors,
                "step3": self.step3_errors,
                "step4": self.step4_errors,
                "step5": self.step5_errors,
            },
            "performance": self.avg_latency_ms,
            "total_processed": sum([self.step1_processed, self.direct_processed]),
            "error_rate": sum([self.step1_errors]) / max(1, self.step1_processed),
            "last_update": self.last_update.isoformat() if self.last_update else None
        }

class PipelineManager:
    """é¡¶é…ç‰ˆæµæ°´çº¿ç®¡ç†å‘˜"""
    
    # ç±»çº§åˆ«å•ä¾‹ï¼ˆå¯é€‰ï¼‰
    _instance: Optional['PipelineManager'] = None
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, brain_callback: Optional[Callable[[Dict], asyncio.Future]] = None, 
                 config: Optional[PipelineConfig] = None):
        
        # é˜²æ­¢é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return
        
        # æ ¸å¿ƒç»„ä»¶
        self.config = config or PipelineConfig()
        self.metrics = PipelineMetrics()
        self.brain_callback = brain_callback
        
        # 5ä¸ªæ­¥éª¤å®ä¾‹ï¼ˆå¸¦ç‹¬ç«‹é”ï¼‰
        self.step1 = Step1Filter()
        self.step2 = Step2Fusion()
        self.step3 = Step3Align()
        self.step4 = Step4Calc()
        self.step5 = Step5CrossCalc()
        
        # æ­¥éª¤é”ï¼ˆé˜²æ­¢å¹¶å‘å¤„ç†åŒç±»å‹æ•°æ®ï¼‰
        self.step_locks = {
            'step1': asyncio.Lock(),
            'step2': asyncio.Lock(),
            'step3': asyncio.Lock(),
            'step4': asyncio.Lock(),
            'step5': asyncio.Lock(),
        }
        
        # æ•°æ®é˜Ÿåˆ—ï¼ˆä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼‰
        self.data_queue = asyncio.PriorityQueue(maxsize=self.config.max_queue_size)
        
        # ç¼“å­˜ï¼ˆLRUé£æ ¼ï¼Œå¸¦è¿‡æœŸæ—¶é—´ï¼‰
        self._step1_cache: List[ExtractedData] = []
        self._step2_cache: List[FusedData] = []
        self._step3_cache: List[AlignedData] = []
        self._step4_cache: List[PlatformData] = []
        
        # æ­»ä¿¡é˜Ÿåˆ—ï¼ˆè®°å½•å¤„ç†å¤±è´¥çš„æ•°æ®ï¼‰
        self.dead_letter_queue: List[Dict[str, Any]] = []
        
        # å»¶è¿Ÿè®°å½•ï¼ˆç”¨äºè®¡ç®—avg_latencyï¼‰
        self._latencies = defaultdict(list)
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
        self._tasks: List[asyncio.Task] = []
        
        # åˆå§‹åŒ–æ—¶é—´
        self._init_time = datetime.now()
        
        logger.info(f"ğŸ›ï¸ æµæ°´çº¿ç®¡ç†å‘˜åˆå§‹åŒ–å®Œæˆ (æ‰¹å¤„ç†é…ç½®: {self.config})")
        self._initialized = True
    
    async def start(self):
        """å¯åŠ¨ç®¡ç†å‘˜ï¼ˆå¯åŠ¨æ‰€æœ‰åå°ä»»åŠ¡ï¼‰"""
        if self._running:
            logger.warning("âš ï¸ ç®¡ç†å‘˜å·²åœ¨è¿è¡Œä¸­")
            return
        
        logger.info("ğŸš€ æµæ°´çº¿ç®¡ç†å‘˜å¯åŠ¨...")
        self._running = True
        
        # å¯åŠ¨æ ¸å¿ƒå¤„ç†å¾ªç¯
        self._tasks = [
            asyncio.create_task(self._data_consumer_loop(), name="consumer"),
            asyncio.create_task(self._metrics_reporter(), name="metrics"),
            asyncio.create_task(self._cleanup_task(), name="cleanup"),
            asyncio.create_task(self._health_check(), name="health")
        ]
        
        logger.info("âœ… æ‰€æœ‰åå°ä»»åŠ¡å¯åŠ¨å®Œæˆ")
    
    async def stop(self, timeout: int = 30):
        """ä¼˜é›…åœæ­¢ï¼ˆç­‰å¾…æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼‰"""
        logger.info(f"ğŸ›‘ ç®¡ç†å‘˜åœæ­¢ä¸­ï¼ˆè¶…æ—¶: {timeout}sï¼‰...")
        self._running = False
        
        # 1. åœæ­¢æ¥æ”¶æ–°æ•°æ®
        await self.data_queue.put((999, {"type": "poison"}))  # æ¯’ä¸¸
        
        # 2. ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        done, pending = await asyncio.wait(self._tasks, timeout=timeout)
        
        if pending:
            logger.warning(f"âš ï¸  {len(pending)} ä¸ªä»»åŠ¡è¶…æ—¶ï¼Œå¼ºåˆ¶å–æ¶ˆ")
            for task in pending:
                task.cancel()
        
        # 3. æ¸…ç†ç¼“å­˜
        self._flush_all_cache()
        
        logger.info("âœ… ç®¡ç†å‘˜å·²åœæ­¢")
    
    async def ingest_data(self, data: Dict[str, Any], priority: int = 5) -> bool:
        """
        æ¥æ”¶åŸå§‹æ•°æ®å…¥å£
        priority: ä¼˜å…ˆçº§(0-10, 0æœ€é«˜)
        """
        try:
            # 1. æ•°æ®åˆ†ç±»
            data_type = data.get("data_type", "")
            classified_type = self._classify_data_type(data_type)
            
            if classified_type is None:
                logger.warning(f"âš ï¸ æ— æ³•åˆ†ç±»çš„æ•°æ®ç±»å‹: {data_type}ï¼Œç›´æ¥ä¼ é€’")
                if self.brain_callback:
                    await self.brain_callback(data)
                return True
            
            # 2. æ ¹æ®ç±»å‹å†³å®šè·¯å¾„
            if classified_type == "market":
                # å¸‚åœºæ•°æ® â†’ å…¥é˜Ÿç­‰å¾…æµæ°´çº¿å¤„ç†
                await self.data_queue.put((priority, {
                    "type": "market",
                    "data": data,
                    "retry_count": 0,
                    "first_seen": time.time()
                }))
                logger.debug(f"ğŸ“¥ å¸‚åœºæ•°æ®å…¥é˜Ÿ: {data_type} {data.get('symbol')}")
                
            elif classified_type == "account":
                # è´¦æˆ·æ•°æ® â†’ ç›´è¿å¤§è„‘
                if self.brain_callback:
                    await self.brain_callback(data)
                self.metrics.direct_processed += 1
                logger.debug(f"ğŸ“¤ è´¦æˆ·æ•°æ®ç›´è¿å¤§è„‘: {data_type}")
            
            elif classified_type == "system":
                # ç³»ç»Ÿæ•°æ® â†’ ç‰¹æ®Šå¤„ç†
                await self._handle_system_data(data)
            
            self.metrics.last_update = datetime.now()
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®å…¥é˜Ÿå¤±è´¥: {e}")
            return False
    
    def _classify_data_type(self, data_type: str) -> Optional[str]:
        """ä¸¥æ ¼åˆ†ç±»æ•°æ®ç±»å‹"""
        market_prefixes = ["ticker", "funding_rate", "mark_price", 
                          "okx_", "binance_"]
        account_prefixes = ["account", "position", "order", "trade"]
        
        # å¸‚åœºæ•°æ®
        if any(data_type.startswith(p) for p in market_prefixes):
            return "market"
        
        # è´¦æˆ·æ•°æ®
        if any(data_type.startswith(p) for p in account_prefixes):
            return "account"
        
        # ç³»ç»Ÿæ•°æ®
        if data_type in ["connection_status", "system"]:
            return "system"
        
        return None
    
    async def _data_consumer_loop(self):
        """æ ¸å¿ƒæ¶ˆè´¹è€…å¾ªç¯ï¼ˆé¡ºåºæ‰§è¡Œæµæ°´çº¿ï¼‰"""
        logger.info("ğŸ”„ æ¶ˆè´¹è€…å¾ªç¯å¯åŠ¨...")
        
        while self._running:
            try:
                # å–æ•°æ®ï¼ˆå¸¦è¶…æ—¶ï¼‰
                priority, item = await asyncio.wait_for(
                    self.data_queue.get(), timeout=1.0
                )
                
                # æ¯’ä¸¸æ£€æŸ¥ï¼ˆåœæ­¢ä¿¡å·ï¼‰
                if item.get("type") == "poison":
                    break
                
                # è®°å½•ç­‰å¾…æ—¶é—´
                wait_time = time.time() - item["first_seen"]
                self._latencies['queue_wait'].append(wait_time * 1000)
                
                # å¤„ç†å•æ¡æ•°æ®
                await self._process_single_item(item)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"æ¶ˆè´¹è€…å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(0.1)
        
        logger.info("ğŸ”„ æ¶ˆè´¹è€…å¾ªç¯åœæ­¢")
    
    async def _process_single_item(self, item: Dict[str, Any]):
        """å¤„ç†å•æ¡æ•°æ®ï¼ˆå®Œæ•´æµæ°´çº¿ï¼‰"""
        raw_data = item["data"]
        retry_count = item["retry_count"]
        
        start_time = time.time()
        
        try:
            # Step1: è¿‡æ»¤æå–
            async with self.step_locks['step1']:
                step1_results = self.step1.process([raw_data])
                self.metrics.step1_processed += len(step1_results)
            
            if not step1_results:
                return
            
            # ç¼“å­˜å¹¶æ£€æŸ¥æ‰¹é‡æ¡ä»¶
            self._step1_cache.extend(step1_results)
            
            # Step2: èåˆï¼ˆæ‰¹é‡ï¼‰
            if len(self._step1_cache) >= self.config.step1_batch_size:
                await self._run_step2()
            
            # Step3: å¯¹é½ï¼ˆæ‰¹é‡ï¼‰
            if len(self._step2_cache) >= self.config.step2_batch_size:
                await self._run_step3()
            
            # Step4: è®¡ç®—ï¼ˆæ‰¹é‡ï¼‰
            if len(self._step3_cache) >= self.config.step3_batch_size:
                await self._run_step4()
            
            # Step5: è·¨å¹³å°è®¡ç®—ï¼ˆæ‰¹é‡ï¼‰
            if len(self._step4_cache) >= self.config.step4_batch_size:
                await self._run_step5()
            
            # è®°å½•æˆåŠŸå»¶è¿Ÿ
            self._record_latency('total', start_time)
            
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            self._record_error(raw_data, e, retry_count)
    
    async def _run_step2(self):
        """æ‰§è¡ŒStep2ï¼ˆå¸¦é”å’Œé”™è¯¯å¤„ç†ï¼‰"""
        async with self.step_locks['step2']:
            try:
                step2_results = self.step2.process(self._step1_cache)
                self.metrics.step2_processed += len(step2_results)
                self._step2_cache.extend(step2_results)
                self._step1_cache.clear()
                logger.debug(f"Step2æ‰¹é‡å®Œæˆ: {len(step2_results)} æ¡")
            except Exception as e:
                self.metrics.step2_errors += 1
                logger.error(f"Step2æ‰¹é‡å¤±è´¥: {e}")
                raise
    
    async def _run_step3(self):
        """æ‰§è¡ŒStep3"""
        async with self.step_locks['step3']:
            try:
                step3_results = self.step3.process(self._step2_cache)
                self.metrics.step3_processed += len(step3_results)
                self._step3_cache.extend(step3_results)
                self._step2_cache.clear()
                logger.debug(f"Step3æ‰¹é‡å®Œæˆ: {len(step3_results)} æ¡")
            except Exception as e:
                self.metrics.step3_errors += 1
                logger.error(f"Step3æ‰¹é‡å¤±è´¥: {e}")
                raise
    
    async def _run_step4(self):
        """æ‰§è¡ŒStep4"""
        async with self.step_locks['step4']:
            try:
                step4_results = self.step4.process(self._step3_cache)
                self.metrics.step4_processed += len(step4_results)
                self._step4_cache.extend(step4_results)
                self._step3_cache.clear()
                logger.debug(f"Step4æ‰¹é‡å®Œæˆ: {len(step4_results)} æ¡")
            except Exception as e:
                self.metrics.step4_errors += 1
                logger.error(f"Step4æ‰¹é‡å¤±è´¥: {e}")
                raise
    
    async def _run_step5(self):
        """æ‰§è¡ŒStep5ï¼ˆæ¨é€ç»™å¤§è„‘ï¼‰"""
        async with self.step_locks['step5']:
            try:
                final_results = self.step5.process(self._step4_cache)
                self.metrics.step5_processed += len(final_results)
                
                # **æ¨é€ç»™å¤§è„‘**
                if self.brain_callback and final_results:
                    for result in final_results:
                        await self.brain_callback(result.__dict__)
                
                self._step4_cache.clear()
                logger.debug(f"Step5æ‰¹é‡å®Œæˆ: {len(final_results)} æ¡ â†’ å¤§è„‘")
            except Exception as e:
                self.metrics.step5_errors += 1
                logger.error(f"Step5æ‰¹é‡å¤±è´¥: {e}")
                raise
    
    def _record_latency(self, step: str, start_time: float):
        """è®°å½•å»¶è¿Ÿ"""
        latency_ms = (time.time() - start_time) * 1000
        self._latencies[step].append(latency_ms)
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        if len(self._latencies[step]) > 100:
            self._latencies[step].pop(0)
        
        self.metrics.avg_latency_ms[step] = sum(self._latencies[step]) / len(self._latencies[step])
    
    def _record_error(self, data: Dict, error: Exception, retry_count: int):
        """è®°å½•é”™è¯¯ï¼ˆæ”¯æŒé‡è¯•ï¼‰"""
        if retry_count < self.config.max_retry_attempts:
            # é‡æ–°å…¥é˜Ÿï¼ˆå¸¦å»¶è¿Ÿï¼‰
            asyncio.create_task(self._retry_with_delay(data, retry_count + 1))
        else:
            # è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
            if self.config.dead_letter_enabled:
                self.dead_letter_queue.append({
                    "data": data,
                    "error": str(error),
                    "timestamp": datetime.now().isoformat()
                })
                logger.error(f"æ•°æ®è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—: {data.get('symbol')} - {error}")
    
    async def _retry_with_delay(self, data: Dict, retry_count: int):
        """å»¶è¿Ÿé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰"""
        delay = 2 ** retry_count  # 2, 4, 8ç§’
        logger.warning(f"â³ {data.get('symbol')} å°†åœ¨ {delay}s åé‡è¯•ï¼ˆç¬¬ {retry_count} æ¬¡ï¼‰")
        await asyncio.sleep(delay)
        await self.ingest_data(data, priority=1)  # é‡è¯•æ•°æ®ä¼˜å…ˆçº§è®¾ä¸º1
    
    async def _handle_system_data(self, data: Dict[str, Any]):
        """å¤„ç†ç³»ç»Ÿæ•°æ®ï¼ˆå¦‚è¿æ¥çŠ¶æ€ï¼‰"""
        # å¯ä»¥åœ¨è¿™é‡Œè®°å½•ç³»ç»ŸçŠ¶æ€åˆ°ç›‘æ§é¢æ¿
        logger.info(f"ğŸ“¡ ç³»ç»Ÿæ•°æ®: {data.get('data_type')} = {data.get('status')}")
    
    async def _metrics_reporter(self):
        """æŒ‡æ ‡æŠ¥å‘Šä»»åŠ¡ï¼ˆå®šæ—¶æ‰“å°ï¼‰"""
        while self._running:
            await asyncio.sleep(self.config.metrics_report_interval)
            
            metrics = self.metrics.to_dict()
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š æµæ°´çº¿è¿è¡ŒæŠ¥å‘Šï¼ˆè¿è¡Œ: {datetime.now() - self._init_time})")
            logger.info(f"å¤„ç†é‡: {metrics['processed']} | é”™è¯¯æ•°: {sum(metrics['errors'].values())}")
            logger.info(f"å»¶è¿Ÿ: {self.metrics.avg_latency_ms.get('total', 0):.2f}ms")
            logger.info(f"æ­»ä¿¡é˜Ÿåˆ—: {len(self.dead_letter_queue)}")
            logger.info("=" * 60)
    
    async def _cleanup_task(self):
        """æ¸…ç†ä»»åŠ¡ï¼ˆå›æ”¶å†…å­˜ã€æ£€æŸ¥æ³„æ¼ï¼‰"""
        while self._running:
            await asyncio.sleep(self.config.cleanup_interval)
            
            # 1. æ£€æŸ¥ç¼“å­˜æ³„æ¼
            total_cached = sum([
                len(self._step1_cache),
                len(self._step2_cache),
                len(self._step3_cache),
                len(self._step4_cache)
            ])
            
            if total_cached > 10000:
                logger.warning(f"âš ï¸  ç¼“å­˜æ€»é‡è¿‡å¤§({total_cached})ï¼Œå¯èƒ½æ³„æ¼")
                self._flush_all_cache()
            
            # 2. å‹ç¼©å»¶è¿Ÿè®°å½•
            for key in self._latencies:
                if len(self._latencies[key]) > 1000:
                    self._latencies[key] = self._latencies[key][-100:]
            
            logger.debug(f"ğŸ§¹ æ¸…ç†å®Œæˆ | ç¼“å­˜: {total_cached} | æ­»ä¿¡: {len(self.dead_letter_queue)}")
    
    async def _health_check(self):
        """å¥åº·æ£€æŸ¥ä»»åŠ¡"""
        while self._running:
            await asyncio.sleep(30)
            
            # æ£€æŸ¥é˜Ÿåˆ—å †ç§¯
            queue_size = self.data_queue.qsize()
            self.metrics.queue_size_history.append(queue_size)
            
            if queue_size > self.config.max_queue_size * 0.8:
                logger.warning(f"âš ï¸  é˜Ÿåˆ—å †ç§¯ä¸¥é‡: {queue_size}/{self.config.max_queue_size}")
            
            # æ£€æŸ¥é”™è¯¯ç‡
            error_rate = self.metrics.step1_errors / max(1, self.metrics.step1_processed)
            if error_rate > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                logger.error(f"ğŸ”¥ é”™è¯¯ç‡è¿‡é«˜: {error_rate*100:.1f}%")
    
    def _flush_all_cache(self):
        """å¼ºåˆ¶æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        logger.warning("ğŸš¨ å¼ºåˆ¶æ¸…ç©ºç¼“å­˜ï¼")
        self._step1_cache.clear()
        self._step2_cache.clear()
        self._step3_cache.clear()
        self._step4_cache.clear()
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çŠ¶æ€ï¼ˆç”¨äºç›‘æ§é¢æ¿ï¼‰"""
        return {
            "running": self._running,
            "uptime_seconds": (datetime.now() - self._init_time).total_seconds(),
            "metrics": self.metrics.to_dict(),
            "pipeline_status": {
                "step1_cache": len(self._step1_cache),
                "step2_cache": len(self._step2_cache),
                "step3_cache": len(self._step3_cache),
                "step4_cache": len(self._step4_cache),
                "queue_size": self.data_queue.qsize(),
                "dead_letter": len(self.dead_letter_queue)
            },
            "config": self.config.__dict__,
            "health": {
                "queue_full_ratio": self.data_queue.qsize() / self.config.max_queue_size,
                "error_rate": self.metrics.step1_errors / max(1, self.metrics.step1_processed)
            }
        }
    
    def get_dead_letters(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æ­»ä¿¡é˜Ÿåˆ—ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return self.dead_letter_queue[-limit:]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    
    async def brain_receive(data: Dict[str, Any]):
        """å¤§è„‘æ¥æ”¶å‡½æ•°"""
        print(f"ğŸ§  æ”¶åˆ°æ•°æ®: {data.get('symbol', 'N/A')} | ç±»å‹: {data.get('data_type', 'N/A')}")
    
    async def main():
        # 1. åˆ›å»ºç®¡ç†å‘˜ï¼ˆå•ä¾‹ï¼‰
        config = PipelineConfig(step1_batch_size=20, cleanup_interval=600)
        manager = PipelineManager(brain_callback=brain_receive, config=config)
        
        # 2. å¯åŠ¨
        await manager.start()
        
        # 3. æ¨¡æ‹Ÿæ•°æ®æµå…¥
        test_data = [
            {"exchange": "binance", "symbol": "BTCUSDT", "data_type": "funding_rate", "raw_data": {"r": 0.0001, "T": 1234567890000}},
            {"exchange": "okx", "symbol": "BTCUSDT", "data_type": "funding_rate", "raw_data": {"instId": "BTC-USDT-SWAP", "fundingRate": 0.0002}},
            {"exchange": "binance", "data_type": "account", "balance": 10000}  # è¿™æ¡ä¼šç›´è¿å¤§è„‘
        ]
        
        for data in test_data:
            await manager.ingest_data(data)
        
        # 4. è¿è¡Œä¸€æ®µæ—¶é—´
        await asyncio.sleep(10)
        
        # 5. è·å–çŠ¶æ€
        print("\n" + "="*60)
        print("æœ€ç»ˆçŠ¶æ€:", manager.get_status())
        print("="*60)
        
        # 6. ä¼˜é›…åœæ­¢
        await manager.stop()
    
    asyncio.run(main())
